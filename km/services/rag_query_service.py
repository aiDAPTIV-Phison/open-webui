"""
RAG Query Service for km-for-agent-builder
æ•´åˆäº† km-for-agent-builder-client çš„æŸ¥è©¢åŠŸèƒ½
"""
import os
import json
from typing import Dict, List, Optional
from loguru import logger
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
import chromadb
from config import settings

# BM25 imports
try:
    from rank_bm25 import BM25Okapi
    import jieba
    BM25_AVAILABLE = True
except ImportError:
    BM25_AVAILABLE = False
    logger.warning("BM25 dependencies not available. Install with: pip install rank-bm25 jieba")


class RAGQueryService:
    """RAG æŸ¥è©¢æœå‹™"""

    def __init__(self):
        self.embedding_model = None
        self.current_model_path = None
        self.collection_cache = {}
        self.bm25_index = None
        self.bm25_documents = []
        # å„ªå…ˆè®€å–ç’°å¢ƒè®Šæ•¸ï¼Œå¦‚æœç‚ºç©ºå†ä½¿ç”¨ settings è¨­å®š
        env_search_algorithm = os.getenv('SEARCH_ALGORITHM', '').strip()
        if env_search_algorithm:
            self.search_algorithm = env_search_algorithm.lower()
            logger.info(f"Using search algorithm from environment variable: {self.search_algorithm}")
        else:
            self.search_algorithm = settings.SEARCH_ALGORITHM.lower()
            logger.info(f"Using search algorithm from settings: {self.search_algorithm}")

        # é©—è­‰æœå°‹æ¼”ç®—æ³•è¨­å®š
        if self.search_algorithm not in ['semantic', 'bm25']:
            logger.warning(f"Invalid search algorithm '{self.search_algorithm}', defaulting to 'semantic'")
            self.search_algorithm = 'semantic'

        if self.search_algorithm == 'bm25' and not BM25_AVAILABLE:
            logger.warning("BM25 requested but not available, falling back to semantic search")
            self.search_algorithm = 'semantic'

    def _init_embedding_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        if self.embedding_model is None:
            try:
                from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings

                # logger.info(f"Loading embedding model from API: {settings.EMBEDDING_URL}")
                # self.embedding_model = HuggingFaceInferenceAPIEmbeddings(
                #     api_url=settings.EMBEDDING_URL,
                #     api_key='empty'
                # )

                from langchain_openai import OpenAIEmbeddings
                # embedding_url = "http://10.101.41.128:13142/v1/"
                logger.info(f"Loading embedding model from API: {settings.EMBEDDING_URL}")
                self.embedding_model = OpenAIEmbeddings(base_url=settings.EMBEDDING_URL, api_key="empty",
                                            tiktoken_enabled=False, check_embedding_ctx_length=False )

                logger.info(f"[SUCCESS] Embedding model loaded from API: {settings.EMBEDDING_URL}")
            except Exception as e:
                logger.exception("[ERROR] Failed to load embedding model from API")
                # å¦‚æœ API è¼‰å…¥å¤±æ•—ï¼Œå›é€€åˆ°æœ¬åœ°æ¨¡å‹
                try:
                    from langchain_community.embeddings import HuggingFaceEmbeddings
                    logger.info("Falling back to local HuggingFace model")
                    self.embedding_model = HuggingFaceEmbeddings(
                        model_name="sentence-transformers/all-MiniLM-L6-v2",
                        model_kwargs={'device': 'cpu'},
                        encode_kwargs={'normalize_embeddings': True}
                    )
                    logger.info("[SUCCESS] Local embedding model loaded as fallback")
                except Exception as fallback_e:
                    logger.exception("[ERROR] Failed to load fallback embedding model")
                    raise fallback_e

    def _get_collection(self, collection_name: str, chroma_path: str = None):
        """ç²å–æˆ–å‰µå»º Chroma collection (å¸¶ç·©å­˜) - åƒè€ƒ km-for-agent-builder-client çš„å¯¦ç¾"""
        if chroma_path is None:
            # å˜—è©¦å¤šå€‹å¯èƒ½çš„è·¯å¾‘
            possible_paths = [
                settings.CHROMA_PATH,  # ä½¿ç”¨é…ç½®ä¸­çš„ CHROMA_PATH
                os.path.join(settings.BASE_FOLDER, "chromadb"),
                os.path.join(settings.BASE_FOLDER, collection_name, "processed_output"),
                os.path.join(settings.BASE_FOLDER, collection_name, "processed_output", "chromadb")
            ]

            chroma_path = None
            for path in possible_paths:
                if os.path.exists(path):
                    chroma_path = path
                    break

            if chroma_path is None:
                # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨é»˜èªè·¯å¾‘
                chroma_path = os.path.join(settings.BASE_FOLDER, collection_name, "processed_output")
                logger.warning(f"No ChromaDB path found, using default: {chroma_path}")

        collection_key = f"{chroma_path}#{collection_name}"

        # å¦‚æœæ˜¯ç›¸åŒçš„ collectionï¼Œç›´æ¥è¿”å›ç·©å­˜çš„å¯¦ä¾‹
        if collection_key in self.collection_cache:
            logger.debug(f"Using cached collection: {collection_name}")
            return self.collection_cache[collection_key]

        # å‰µå»ºæ–°çš„ collection
        logger.info(f"Loading new collection: {collection_name} from path: {chroma_path}")
        if self.embedding_model is None:
            self._init_embedding_model()

        try:
            # åƒè€ƒ km-for-agent-builder-client çš„å¯¦ç¾æ–¹å¼
            collection = Chroma(
                persist_directory=chroma_path,
                embedding_function=self.embedding_model,
                collection_name=collection_name
            )
            self.collection_cache[collection_key] = collection
            logger.info(f"Successfully loaded collection: {collection_name}")
            return collection
        except Exception as e:
            logger.error(f"Failed to load collection {collection_name} from {chroma_path}: {str(e)}")
            raise e

    def get_available_collections(self) -> List[str]:
        """ç²å–å¯ç”¨çš„ collection åˆ—è¡¨ - ç›´æ¥å¾ ChromaDB ä¸­æŸ¥æ‰¾"""
        collections = []

        try:
            # æ§‹å»º ChromaDB çš„åŸºç¤è·¯å¾‘
            chroma_base_path = os.path.join(settings.CHROMA_PATH)

            # å¦‚æœ ChromaDB ç›®éŒ„ä¸å­˜åœ¨ï¼Œå˜—è©¦å…¶ä»–å¯èƒ½çš„è·¯å¾‘
            if not os.path.exists(chroma_base_path):
                # å˜—è©¦åœ¨æ¯å€‹ collection ç›®éŒ„ä¸‹æŸ¥æ‰¾ processed_output/chromadb
                base_folder = settings.BASE_FOLDER
                if os.path.exists(base_folder):
                    for item in os.listdir(base_folder):
                        item_path = os.path.join(base_folder, item)
                        if os.path.isdir(item_path):
                            # æª¢æŸ¥æ˜¯å¦æœ‰ processed_output ç›®éŒ„
                            processed_output_path = os.path.join(item_path, "processed_output")
                            if os.path.exists(processed_output_path):
                                # æª¢æŸ¥æ˜¯å¦æœ‰ chromadb ç›®éŒ„
                                chroma_path = os.path.join(processed_output_path, "chromadb")
                                if os.path.exists(chroma_path):
                                    chroma_base_path = chroma_path
                                    break

            logger.info(f"Searching for collections in ChromaDB path: {chroma_base_path}")

            if not os.path.exists(chroma_base_path):
                logger.warning(f"ChromaDB path does not exist: {chroma_base_path}")
                return collections

            # ä½¿ç”¨ ChromaDB å®¢æˆ¶ç«¯ç›´æ¥æŸ¥è©¢ collections
            try:
                # å‰µå»º ChromaDB å®¢æˆ¶ç«¯
                client = chromadb.PersistentClient(path=chroma_base_path)

                # ç²å–æ‰€æœ‰ collections
                chroma_collections = client.list_collections()

                for collection in chroma_collections:
                    collection_name = collection.name
                    # æª¢æŸ¥ collection æ˜¯å¦æœ‰æ•¸æ“š
                    try:
                        count = collection.count()
                        if count > 0:
                            collections.append(collection_name)
                            logger.info(f"Found collection '{collection_name}' with {count} documents")
                        else:
                            logger.debug(f"Collection '{collection_name}' is empty, skipping")
                    except Exception as e:
                        logger.warning(f"Error checking collection '{collection_name}': {str(e)}")
                        # å³ä½¿ç„¡æ³•æª¢æŸ¥æ•¸é‡ï¼Œä¹Ÿå˜—è©¦æ·»åŠ ï¼ˆå¯èƒ½ collection å­˜åœ¨ä½†ç„¡æ³•è¨ªå•ï¼‰
                        collections.append(collection_name)

                logger.info(f"Found {len(collections)} available collections from ChromaDB: {collections}")

            except Exception as chroma_error:
                logger.error(f"Error accessing ChromaDB: {str(chroma_error)}")
                # å¦‚æœ ChromaDB è¨ªå•å¤±æ•—ï¼Œå›é€€åˆ°æ–‡ä»¶ç³»çµ±æª¢æŸ¥
                logger.info("Falling back to file system check...")
                return self._get_collections_from_filesystem()

        except Exception as e:
            logger.error(f"Error getting available collections: {str(e)}")
            return []

        return collections

    def _get_collections_from_filesystem(self) -> List[str]:
        """å¾æ–‡ä»¶ç³»çµ±ç²å– collectionsï¼ˆå‚™ç”¨æ–¹æ³•ï¼‰"""
        collections = []
        base_folder = settings.BASE_FOLDER

        if not os.path.exists(base_folder):
            return collections

        try:
            for item in os.listdir(base_folder):
                item_path = os.path.join(base_folder, item)
                if os.path.isdir(item_path):
                    # æª¢æŸ¥æ˜¯å¦æœ‰ processed_output ç›®éŒ„
                    processed_output_path = os.path.join(item_path, "processed_output")
                    if os.path.exists(processed_output_path):
                        # æª¢æŸ¥æ˜¯å¦æœ‰ chunks.json æ–‡ä»¶
                        chunks_file = os.path.join(processed_output_path, "chunks.json")
                        if os.path.exists(chunks_file):
                            collections.append(item)

            logger.info(f"Found {len(collections)} collections from filesystem: {collections}")
            return collections
        except Exception as e:
            logger.error(f"Error getting collections from filesystem: {str(e)}")
            return []

    def clear_collection_cache(self, collection_name: str = None):
        """æ¸…é™¤ collection ç·©å­˜ - åƒè€ƒ km-for-agent-builder-client çš„å¯¦ç¾"""
        if collection_name:
            # åªæ¸…é™¤ç‰¹å®š collection çš„ç·©å­˜
            keys_to_remove = [key for key in self.collection_cache.keys() if collection_name in key]
            for key in keys_to_remove:
                del self.collection_cache[key]
            logger.info(f"Cleared collection cache for: {collection_name}")
        else:
            # æ¸…é™¤æ‰€æœ‰ collection ç·©å­˜
            self.collection_cache.clear()
            logger.info("Cleared all collection caches")

    def _tokenize_text(self, text: str) -> List[str]:
        """æ–‡æœ¬åˆ†è© - æ”¯æ´ä¸­è‹±æ–‡"""
        if not BM25_AVAILABLE:
            return text.split()

        # ä½¿ç”¨ jieba é€²è¡Œä¸­æ–‡åˆ†è©
        tokens = jieba.lcut(text)
        # éæ¿¾æ‰ç©ºç™½å’Œæ¨™é»ç¬¦è™Ÿ
        tokens = [token.strip() for token in tokens if token.strip() and len(token.strip()) > 1]
        return tokens

    def _init_bm25_index(self, collection_name: str):
        """åˆå§‹åŒ– BM25 ç´¢å¼•"""
        if not BM25_AVAILABLE:
            logger.error("BM25 not available")
            return False

        try:
            # ç²å– ChromaDB collection
            chroma = self._get_collection(collection_name)

            # ç²å–æ‰€æœ‰æ–‡æª”
            all_docs = chroma._collection.get()
            documents = all_docs['documents']
            metadatas = all_docs['metadatas']

            if not documents:
                logger.warning(f"No documents found in collection {collection_name}")
                return False

            # ç‚ºæ¯å€‹æ–‡æª”å‰µå»ºåˆ†è©å¾Œçš„æ–‡æœ¬
            tokenized_docs = []
            self.bm25_documents = []

            for i, doc in enumerate(documents):
                # çµåˆæ–‡æª”å…§å®¹å’Œå…ƒæ•¸æ“šé€²è¡Œåˆ†è©
                full_text = doc
                if metadatas and i < len(metadatas) and metadatas[i]:
                    metadata = metadatas[i]
                    if 'source' in metadata:
                        full_text += f" {metadata['source']}"

                tokenized_doc = self._tokenize_text(full_text)
                tokenized_docs.append(tokenized_doc)
                self.bm25_documents.append({
                    'content': doc,
                    'metadata': metadatas[i] if metadatas and i < len(metadatas) else {},
                    'tokens': tokenized_doc
                })

            # å‰µå»º BM25 ç´¢å¼•
            self.bm25_index = BM25Okapi(tokenized_docs)
            logger.info(f"BM25 index created for collection {collection_name} with {len(documents)} documents")
            return True

        except Exception as e:
            logger.error(f"Failed to initialize BM25 index: {str(e)}")
            return False

    def _bm25_search(self, collection_name: str, question: str, k: int = 5) -> List[Dict]:
        """ä½¿ç”¨ BM25 é€²è¡Œæœå°‹"""
        if not BM25_AVAILABLE:
            logger.error("BM25 not available")
            return []

        try:
            # å¦‚æœç´¢å¼•ä¸å­˜åœ¨ï¼Œå…ˆåˆå§‹åŒ–
            if self.bm25_index is None:
                if not self._init_bm25_index(collection_name):
                    return []

            # å°æŸ¥è©¢é€²è¡Œåˆ†è©
            query_tokens = self._tokenize_text(question)

            # ä½¿ç”¨ BM25 é€²è¡Œæœå°‹
            scores = self.bm25_index.get_scores(query_tokens)

            # ç²å–å‰ k å€‹çµæœ
            top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]

            results = []
            for idx in top_indices:
                logger.info(f"BM25 score: {idx} {scores[idx]}")
                if scores[idx] > 0:  # åªè¿”å›æœ‰åˆ†æ•¸çš„çµæœ
                    doc_info = self.bm25_documents[idx]
                    results.append({
                        'content': doc_info['content'],
                        'metadata': doc_info['metadata'],
                        'score': scores[idx]
                    })

            logger.info(f"BM25 search returned {len(results)} results")
            return results

        except Exception as e:
            logger.error(f"BM25 search failed: {str(e)}")
            return []

    def get_rag_context_with_file_content(self, collection_name: str, question: str, k: int = 5) -> Dict:
        """
        æ ¹æ“šå•é¡Œå¾ chroma æª¢ç´¢ç›¸é—œå…§å®¹ï¼Œä¸¦å¾å°æ‡‰çš„ merged file ä¸­è®€å–å®Œæ•´å…§å®¹ä¾†æ§‹å»ºèŠå¤©æ¶ˆæ¯

        Args:
            collection_name: é›†åˆåç¨±
            question: ç”¨æˆ¶å•é¡Œ
            k: æª¢ç´¢çš„ top-k æ•¸é‡

        Returns:
            dict: {
                'filename': str,  # é¸ä¸­çš„æ–‡ä»¶å
                'file_path': str,  # æ–‡ä»¶çš„å®Œæ•´è·¯å¾‘
                'chat_messages': List[dict],  # æ¨ç†çš„èŠå¤©æ¶ˆæ¯åˆ—è¡¨
                'merged_content': str,  # åˆä½µçš„å…§å®¹
                'error': str  # éŒ¯èª¤ä¿¡æ¯ï¼ŒæˆåŠŸæ™‚ç‚ºç©ºå­—ç¬¦ä¸²
            }
        """
        try:
            # ç²å– collection - è®“ _get_collection è‡ªå‹•å°‹æ‰¾æ­£ç¢ºçš„è·¯å¾‘
            chroma = self._get_collection(collection_name)

            # èª¿è©¦ä¿¡æ¯ï¼šæª¢æŸ¥ collection ç‹€æ…‹
            try:
                collection_count = chroma._collection.count()
                logger.info(f"Collection '{collection_name}' contains {collection_count} documents")
            except Exception as e:
                logger.warning(f"Could not get collection count: {str(e)}")

            # æ ¹æ“šè¨­å®šçš„æ¼”ç®—æ³•é€²è¡Œæœå°‹
            logger.info(f"Searching for: '{question}' with k={k} using {self.search_algorithm} algorithm")

            if self.search_algorithm == 'bm25':
                # ä½¿ç”¨ BM25 æœå°‹
                bm25_results = self._bm25_search(collection_name, question, k)
                if not bm25_results:
                    return {
                        'filename': None,
                        'file_path': None,
                        'chat_messages': [],
                        'merged_content': '',
                        'error': 'no BM25 search results found'
                    }

                # è½‰æ› BM25 çµæœæ ¼å¼ä»¥åŒ¹é…èªæ„æœå°‹çš„æ ¼å¼
                results = []
                for result in bm25_results:
                    # å‰µå»ºé¡ä¼¼ Document çš„å°è±¡
                    class MockDocument:
                        def __init__(self, content, metadata):
                            self.page_content = content
                            self.metadata = metadata

                    mock_doc = MockDocument(result['content'], result['metadata'])
                    results.append((mock_doc, result['score']))

            else:
                # ä½¿ç”¨èªæ„æœå°‹ï¼ˆé»˜èªï¼‰
                results = chroma.similarity_search_with_score(question, k=k)

            logger.info(f"Search returned {len(results)} results")

            if not results:
                return {
                    'filename': None,
                    'file_path': None,
                    'chat_messages': [],
                    'merged_content': '',
                    'error': 'no search results found'
                }

            # çµ±è¨ˆæ¯å€‹ group_id çš„å‡ºç¾æ¬¡æ•¸å’Œç›¸ä¼¼åº¦ç¸½å’Œ
            group_stats = {}
            all_chunks = []

            for doc, score in results:
                group_id = doc.metadata.get('group_id', '')
                chunk_content = doc.page_content
                all_chunks.append(chunk_content)

                if group_id:
                    if group_id not in group_stats:
                        group_stats[group_id] = {
                            'count': 0,
                            'similarity_sum': 0.0,
                            'scores': [],
                            'chunks': []
                        }

                    group_stats[group_id]['count'] += 1
                    group_stats[group_id]['similarity_sum'] += score
                    group_stats[group_id]['scores'].append(score)
                    group_stats[group_id]['chunks'].append(chunk_content)
                    logger.info(f"group_id: {group_id}, similarity_sum: {group_stats[group_id]['similarity_sum']}, scores: {group_stats[group_id]['scores']}")

            logger.info(f"group_stats: {group_stats}")
            if not group_stats:
                return {
                    'filename': None,
                    'file_path': None,
                    'chat_messages': [],
                    'merged_content': '',
                    'error': 'no valid group_ids found'
                }

            # é¸æ“‡æœ€ç›¸é—œçš„ group_id
            max_count = max(stats['count'] for stats in group_stats.values())
            top_groups = [group_id for group_id, stats in group_stats.items()
                         if stats['count'] == max_count]

            if len(top_groups) == 1:
                selected_group_id = top_groups[0]
            else:
                best_group = None
                best_similarity_sum = float('inf')

                for group_id in top_groups:
                    similarity_sum = group_stats[group_id]['similarity_sum']
                    if similarity_sum < best_similarity_sum:
                        best_similarity_sum = similarity_sum
                        best_group = group_id

                selected_group_id = best_group

            logger.info(f"Selected group_id: {selected_group_id}")

            # æ§‹å»º merge file åç¨±ä¸¦å°‹æ‰¾å¯¦éš›å­˜åœ¨çš„æ–‡ä»¶
            # ä½¿ç”¨ selected_group_id ä½œç‚º merged file çš„åŸºç¤åç¨±
            group_filename = selected_group_id

            # æ§‹å»ºå¯èƒ½çš„ merged file è·¯å¾‘
            base_paths = [
                os.path.join(settings.BASE_FOLDER, collection_name, "merged_files"),
                os.path.join(settings.BASE_FOLDER, collection_name, "processed_output", "merged_files"),
                os.path.join("tmp", collection_name, "processed_output", "merged_files")
            ]

            merged_file_path = None
            merged_file_name = None

            # å˜—è©¦ä¸åŒçš„æ–‡ä»¶åæ¨¡å¼
            possible_names = [
                f"{group_filename}.txt",
                f"{group_filename}_merged_part1.txt",
                f"{group_filename}_merged.txt"
            ]

            # åœ¨æ¯å€‹å¯èƒ½çš„åŸºç¤è·¯å¾‘ä¸­å°‹æ‰¾æ–‡ä»¶
            for base_path in base_paths:
                if os.path.exists(base_path):
                    for name in possible_names:
                        potential_path = os.path.join(base_path, name)
                        if os.path.exists(potential_path):
                            merged_file_path = potential_path
                            merged_file_name = name
                            logger.info(f"Found merged file: {merged_file_path}")
                            break
                    if merged_file_path:
                        break

            # å¦‚æœéƒ½æ²’æ‰¾åˆ°ï¼Œä½¿ç”¨ç¬¬ä¸€å€‹è·¯å¾‘å’Œç¬¬ä¸€å€‹æ–‡ä»¶åä½œç‚ºé»˜èª
            if merged_file_path is None:
                merged_file_name = possible_names[0]
                merged_file_path = os.path.join(base_paths[0], merged_file_name)
                logger.info(f"Using default merged file path: {merged_file_path}")
            else:
                logger.info(f"Selected merge filename: {merged_file_name}")

            # å¾æŒ‡å®šçš„ txt æª”æ¡ˆä¸­è®€å–å…§å®¹
            merged_content = ""
            try:
                logger.info(f"Attempting to read merged file: {merged_file_path}")

                if os.path.exists(merged_file_path):
                    with open(merged_file_path, 'r', encoding='utf-8') as f:
                        merged_content = f.read().strip()
                    logger.info(f"Successfully read merged file, content length: {len(merged_content)} chars")
                else:
                    logger.warning(f"Merged file not found: {merged_file_path}")
                    # å¦‚æœæ‰¾ä¸åˆ° merged fileï¼Œä½¿ç”¨æª¢ç´¢åˆ°çš„ chunks
                    merged_content = "\n\n".join(all_chunks)
                    logger.info(f"Using retrieved chunks as fallback, content length: {len(merged_content)} chars")

            except Exception as file_error:
                logger.error(f"Failed to read merged file: {str(file_error)}")
                merged_content = "\n\n".join(all_chunks)
                logger.info(f"Using retrieved chunks as fallback due to error, content length: {len(merged_content)} chars")

            # å‰µå»ºç”¨æ–¼æ¨ç†çš„èŠå¤©æ¶ˆæ¯
            chat_messages = []

            # å¦‚æœ system prompt ä¸ç‚ºç©ºï¼Œå‰‡æ·»åŠ  system æ¶ˆæ¯
            if settings.SYSTEM_PROMPT and settings.SYSTEM_PROMPT.strip():
                chat_messages.append({
                    "role": "system",
                    "content": settings.SYSTEM_PROMPT
                })

            # æ·»åŠ  user æ¶ˆæ¯
            user_prompt_template = settings.USER_PROMPT_TEMPLATE
            user_content = user_prompt_template.format(chunk=merged_content, query=question)
            chat_messages.append({
                "role": "user",
                "content": user_content
            })

            logger.info(f"Suggested merge file name: {merged_file_name if merged_file_name else f'{group_filename}.txt'}")
            logger.info(f"Generated {len(chat_messages)} chat messages")
            logger.debug(f"Retrieved {len(all_chunks)} document chunks")

            return {
                'filename': merged_file_name if merged_file_name else f"{group_filename}.txt",
                'file_path': merged_file_path,
                'chat_messages': chat_messages,
                'merged_content': merged_content,
                'error': ''
            }

        except Exception as e:
            logger.error(f"get_rag_context_with_file_content error: {str(e)}")
            return {
                'filename': None,
                'file_path': None,
                'chat_messages': [],
                'merged_content': '',
                'error': f'internal error: {str(e)}'
            }

    def generate_openai_payload(self, collection_name: str, query: str, k: int = 5,
                               stream: bool = True, model: str = "gpt-4",
                               params: Optional[Dict] = None) -> Dict:
        """
        ç”Ÿæˆæ¨™æº– OpenAI æ ¼å¼çš„ payload

        Args:
            collection_name: é›†åˆåç¨±
            query: ç”¨æˆ¶å•é¡Œ
            k: æª¢ç´¢çš„ top-k æ•¸é‡
            stream: æ˜¯å¦æµå¼è¼¸å‡º
            model: æ¨¡å‹åç¨±
            params: é¡å¤–åƒæ•¸

        Returns:
            dict: {
                'success': bool,
                'payload_raw': str,
                'message': str
            }
        """
        try:
            # ç²å– RAG ä¸Šä¸‹æ–‡
            result = self.get_rag_context_with_file_content(collection_name, query, k)

            if not result.get("success", True) or result.get("error"):
                return {
                    'success': False,
                    'payload_raw': '',
                    'message': result.get("error", "Failed to get RAG context")
                }

            # æå–æ–‡ä»¶å
            filename = result.get("filename", "")
            filename_wo_ext = os.path.splitext(filename)[0] if filename else ""

            # æ§‹å»ºæ¨™æº–çš„ OpenAI æ ¼å¼ payload
            messages = []

            # æ·»åŠ  system message
            if settings.SYSTEM_PROMPT and settings.SYSTEM_PROMPT.strip():
                messages.append({
                    "role": "system",
                    "content": settings.SYSTEM_PROMPT
                })

            # æ·»åŠ ç”¨æˆ¶æ¶ˆæ¯
            user_content = settings.USER_PROMPT_TEMPLATE.format(
                chunk=result.get("merged_content", ""),
                query=query
            )
            messages.append({
                "role": "user",
                "content": user_content
            })

            # æ§‹å»º payload å°è±¡
            payload_obj = {
                "stream": stream,
                "model": model,
                "messages": messages,
                "max_tokens": params.get("max_tokens", 2048) if params else 2048,
                "temperature": params.get("temperature", 0.7) if params else 0.7,
                "top_p": params.get("top_p", 1.0) if params else 1.0,
                "debug_llm_payload": {
                    "km_service_used": True,
                    "collection": collection_name,
                    "filename": filename_wo_ext,
                    "original_query": query,
                    "rag_content_length": len(result.get("merged_content", ""))
                }
            }

            # è½‰æ›ç‚º JSON å­—ç¬¦ä¸²
            payload_raw = json.dumps(payload_obj, ensure_ascii=False)

            return {
                'success': True,
                'payload_raw': payload_raw,
                'message': 'OpenAI payload generated successfully'
            }

        except Exception as e:
            logger.error(f"Error generating OpenAI payload: {e}")
            return {
                'success': False,
                'payload_raw': '',
                'message': f"Internal error: {str(e)}"
            }

if __name__ == '__main__':
    # åœ¨æ¸¬è©¦æ¨¡å¼ä¸‹ä½¿ç”¨ 64 ç¶­çš„å‡åµŒå…¥æ¨¡å‹
    class TestFakeEmbeddings:
        def __init__(self, *args, **kwargs):
            pass

        def embed_documents(self, texts):
            # Deterministic pseudo-embeddings based on text length
            import numpy as np
            rng = np.random.default_rng(42)
            vectors = []
            for t in texts:
                length = max(1, len(t))
                rng_local = np.random.default_rng(length)
                vec = rng_local.normal(size=64)  # 64 ç¶­ï¼Œèˆ‡æ¸¬è©¦è…³æœ¬ä¸€è‡´
                # L2 normalize
                norm = (vec**2).sum() ** 0.5
                if norm != 0:
                    vec = vec / norm
                vectors.append(vec.tolist())
            return vectors

        def embed_query(self, text):
            return self.embed_documents([text])[0]

    # å‰µå»º RAG æŸ¥è©¢æœå‹™ä¸¦æ›¿æ›åµŒå…¥æ¨¡å‹
    rag_query_service = RAGQueryService()
    rag_query_service.embedding_model = TestFakeEmbeddings()
    print(f"ğŸ§ª æ¸¬è©¦æ¨¡å¼ï¼šä½¿ç”¨ 64 ç¶­å‡åµŒå…¥æ¨¡å‹ï¼Œæœå°‹æ¼”ç®—æ³•ï¼š{rag_query_service.search_algorithm.upper()}")

    collections = rag_query_service.get_available_collections()
    print(f"Available collections: {collections}")

    # ç°¡å–®çš„ RAG æŸ¥è©¢æ¸¬è©¦
    if collections:
        test_collection = collections[0]
        print(f"\næ¸¬è©¦ RAG æŸ¥è©¢ - Collection: {test_collection}")

        # å…ˆæª¢æŸ¥ collection ç‹€æ…‹
        try:
            chroma = rag_query_service._get_collection(test_collection)
            count = chroma._collection.count()
            print(f"Collection æ–‡æª”æ•¸é‡: {count}")
        except Exception as e:
            print(f"âš ï¸  ç„¡æ³•ç²å– collection ç‹€æ…‹: {str(e)}")

        result = rag_query_service.get_rag_context_with_file_content(
            collection_name=test_collection,
            question="what is NVM ExpressTM",
            k=3
        )

        if result.get('error'):
            print(f"âŒ æŸ¥è©¢å¤±æ•—: {result['error']}")
        else:
            print(f"âœ… æŸ¥è©¢æˆåŠŸ")
            print(f"   æ¨è–¦æ–‡ä»¶: {result.get('filename', 'N/A')}")
            print(f"   æ¶ˆæ¯æ•¸é‡: {len(result.get('chat_messages', []))}")
            # print(result)

        # æ¸¬è©¦ generate_openai_payload åŠŸèƒ½
        print(f"\n=== æ¸¬è©¦ OpenAI Payload ç”Ÿæˆ ===")
        try:
            openai_result = rag_query_service.generate_openai_payload(
                collection_name=test_collection,
                query="what is NVM ExpressTM",
                k=3,
                stream=False,
                model="gpt-3.5-turbo",
                params={"temperature": 0.7, "max_tokens": 1000}
            )

            if openai_result['success']:
                print(f"âœ… OpenAI Payload ç”ŸæˆæˆåŠŸ")
                print(f"   æ¶ˆæ¯: {openai_result['message']}")
                print(f"   Payload é•·åº¦: {len(openai_result['payload_raw'])} å­—ç¬¦")

                # é¡¯ç¤º payload å…§å®¹ï¼ˆå‰ 500 å­—ç¬¦ï¼‰
                payload_preview = openai_result['payload_raw'][:500]
                print(f"   Payload é è¦½: {payload_preview}...")

                # å˜—è©¦è§£æ JSON ä¾†é©—è­‰æ ¼å¼
                try:
                    import json
                    payload_obj = json.loads(openai_result['payload_raw'])
                    print(f"   âœ… JSON æ ¼å¼é©—è­‰é€šé")
                    print(f"   æ¨¡å‹: {payload_obj.get('model', 'N/A')}")
                    print(f"   æµå¼: {payload_obj.get('stream', 'N/A')}")
                    print(f"   æ¶ˆæ¯æ•¸é‡: {len(payload_obj.get('messages', []))}")
                except json.JSONDecodeError as e:
                    print(f"   âŒ JSON æ ¼å¼éŒ¯èª¤: {str(e)}")
            else:
                print(f"âŒ OpenAI Payload ç”Ÿæˆå¤±æ•—: {openai_result['message']}")

        except Exception as e:
            print(f"âŒ OpenAI Payload æ¸¬è©¦å¤±æ•—: {str(e)}")
    else:
        print("\nâš ï¸  æ²’æœ‰å¯ç”¨çš„ collections")

